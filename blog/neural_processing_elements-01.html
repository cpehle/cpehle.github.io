<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Neural Processing Elements</title>
  <style>
    .csl-left-margin {
      float: left
    }
    .csl-right-inline {
      padding-left: 0.5em;
      float: left
    }
    .csl-entry {
      display: flex;
    }
    .references {
      font-size: 10px;
    }
  </style>
<!--  
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>

-->
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
<link rel="stylesheet" href="https://unpkg.com/tachyons/css/tachyons.min.css">
</head>
<body>
<nav class="pa3 pa4-ns sans-serif">
    <a class="link dim black b f6 f5-ns dib mr3" href="/" title="Home">About</a>
    <a class="link dim gray    f6 f5-ns dib mr3" href="/#projects" title="Projects">Projects</a>
    <a class="link dim gray    f6 f5-ns dib" href="/#contact" title="Contact">Contact</a>
</nav>
<article class="pa3 pa4-ns black-80 serif lh-copy">
<header id="title-block-header">
<h1 class="f2 sans-serif">Neural Processing Elements</h1>
</header>
<section class="measure-wide">
<p>Note: The following text almost entirely coincides with parts of
chapter 2 of my PhD thesis. I am republishing it here because I want to
rework it in a more accessible form and because it is a good case study
for the tooling needed to get a “nice” blogging setup.</p>
<hr />
<p>The nervous system has besides its complicated dynamics a particular
structure. The working hypothesis of <em>connectionism</em> is that how
the primitive components used to model neurons are connected that is its
structure already in large part determines the (potential) function. The
question then becomes to what degree of fidelity the structure of the
biological nervous system needs to be understood and captured in order
to derive useful functional models. Arguably the most important
characteristic of of biological nervous systems is their capability to
“adapt” or “learn” beyond the innate function they had at birth.</p>
<p>In this article I describe a framework based on <em>category
theory</em>, which allows for the description of self-optimizing
“machines”, which I call “Neural Processing Elements”. Those machines
can be composed and nested to form larger machines. The category theory
perspective on “wiring diagrams” has been advanced in several papers
<span class="citation"
data-cites="zardini2020compositional lerman2020networks spivak:2013"> [<a
href="#ref-zardini2020compositional" role="doc-biblioref">1</a>–<a
href="#ref-spivak:2013" role="doc-biblioref">3</a>]</span>. In some ways
the description I give here is just a different perspective on well
known results in the literature, which I will point out along the way.
Furthermore I will not actually give any formal defintions here. This
would require the introduction of far more background material. Rather
the ideas I present here informed my thinking on the problem of
self-optimizing systems.</p>
<p>While I want to consider a more general case later, let me first
discuss artificial neural networks. In the case of artificial neural
networks advances in their computational power significantly derived
from structural innovations. While it is well known that a two layer
artificial neural network is in principle already capable of
approximating a large class of functions, in practice restrictions on
connectivity and parameter sharing as in convolutional architectures,
residual neural networks <span class="citation"
data-cites="he2016deep"> [<a href="#ref-he2016deep"
role="doc-biblioref">4</a>]</span>, as well as transformer and attention
architectures <span class="citation"
data-cites="vaswani2017attention"> [<a href="#ref-vaswani2017attention"
role="doc-biblioref">5</a>]</span>, demonstrate the large impact that
choice of structures have.</p>
<p>Moreover choice of structure or network architecture both has an
impact on the feasability of solving a given task, as well as the
convergence speed of the training algorithm. A basic insight is that the
choice of structure can already ensure that a network can be trained to
accomplish a wide range of domain specific tasks (vision, natural
language processing, etc.).</p>
<p>The main enabling innovation in the case of artificial neural
networks is the backpropagation algorithm <span class="citation"
data-cites="linnainmaa:1970 linnainmaa:1976 rumelhart:86"> [<a
href="#ref-linnainmaa:1970" role="doc-biblioref">6</a>–<a
href="#ref-rumelhart:86" role="doc-biblioref">8</a>]</span>. Briefly
speaking it answers the question how to <em>efficiently</em> compute
derivatives of a function composed out of a set of primitive functions
with respect to parameters, in the case that the space of parameters has
much higher dimensionality than the co-domain of the function. The basic
insight is that this problem can be reduced to a two-phase message
passing algorithm: By assumption the function can be computed by a
directed acyclic graph whose nodes are primitives <span
class="math inline">\phi</span> and edges are variables. During the
<em>forward pass</em> values are propagated along these edges to
ultimately compute the function, while simultaneously the input values
to each node (or more generally a <em>context</em>) is recorded. During
the <em>backward pass</em> cotangent vectors are propagated and at each
node the <span class="math inline">\phi^*</span> with respect to the
given primitive at the input computed during the forward pass is
computed. While this algorithm lends itself well to bulk-synchronous
parallel implementations (c.f.<span class="citation"
data-cites="tensorflow2015-whitepaper"> [<a
href="#ref-tensorflow2015-whitepaper" role="doc-biblioref">9</a>]</span>
and references therein) it exhibits inherent forward-backward locking in
that the backward phase depends on values computed during the forward
phase.</p>
<figure>
<img src="figures/pe_forward_backward.png" id="fig:pe_forward_backward"
alt="Figure 1: Composition of two processing elements e_1, e_2 computing primitives \phi_1 \colon X \to Y and \phi_2 \colon Y \to Z in sequence. Processing happens time steps, t_0, t_1 = t_0 + \delta t, \ldots. For simplicity we illustrate the case here, where every processing element takes one time step to compute its result. During the forward pass a message with value x \in X (solid green circle) is passed into e_1 at t_0, in the next timestep t_1 it is stored in the associated memory element m_1 and an input message with value y = \phi_1(x) \in Y (solid red circle) to e_2 is produced, moreover processing element e_1 does not accept new messages until the value in m_1 is consumed. The processing element e_2 stores this message at timestep t_2 in m_2 and produces an output message to the next processing elements with value z = \phi_2(y) \in Z (solid blue circle). During the backward pass, messages are passed in reverse order through the directed acyclic graph. This happens because all processing elements wait for both a valid value in their associated memory element and a valid error message. For example element e^*_2, which computes the pullback \phi^*_2, waits for an error message dz \in T^*_{z} Z (open blue circle) and a valid value in the memory element m_2 before it produces an output error message with value dy = (\phi_2^*)_y(dz) \in T^*_{y} Y (open red circle) passed on to e^*_1, which computes dx = (\phi^*_1)_x(dy) \in T^*_x X (open green circle)." />
<figcaption aria-hidden="true">Figure 1: Composition of two processing
elements <span class="math inline">e_1, e_2</span> computing primitives
<span class="math inline">\phi_1 \colon X \to Y</span> and <span
class="math inline">\phi_2 \colon Y \to Z</span> in sequence. Processing
happens time steps, <span class="math inline">t_0, t_1 = t_0 + \delta t,
\ldots</span>. For simplicity we illustrate the case here, where every
processing element takes one time step to compute its result. During the
forward pass a message with value <span class="math inline">x \in
X</span> (solid green circle) is passed into <span
class="math inline">e_1</span> at <span class="math inline">t_0</span>,
in the next timestep <span class="math inline">t_1</span> it is stored
in the associated memory element <span class="math inline">m_1</span>
and an input message with value <span class="math inline">y = \phi_1(x)
\in Y</span> (solid red circle) to <span class="math inline">e_2</span>
is produced, moreover processing element <span
class="math inline">e_1</span> does not accept new messages until the
value in <span class="math inline">m_1</span> is consumed. The
processing element <span class="math inline">e_2</span> stores this
message at timestep <span class="math inline">t_2</span> in <span
class="math inline">m_2</span> and produces an output message to the
next processing elements with value <span class="math inline">z =
\phi_2(y) \in Z</span> (solid blue circle). During the backward pass,
messages are passed in reverse order through the directed acyclic graph.
This happens because all processing elements wait for both a valid value
in their associated memory element and a valid error message. For
example element <span class="math inline">e^*_2</span>, which computes
the pullback <span class="math inline">\phi^*_2</span>, waits for an
error message <span class="math inline">dz \in T^*_{z} Z</span> (open
blue circle) and a valid value in the memory element <span
class="math inline">m_2</span> before it produces an output error
message with value <span class="math inline">dy = (\phi_2^*)_y(dz) \in
T^*_{y} Y</span> (open red circle) passed on to <span
class="math inline">e^*_1</span>, which computes <span
class="math inline">dx = (\phi^*_1)_x(dy) \in T^*_x X</span> (open green
circle).</figcaption>
</figure>
<p>From a programming language perspective a curious feature of
artificial neural network models is that their architecture can
typically be described very concisely in terms of few repeating and
nested primitives, together with task specific parameters. This enables
the development of hardware accelerators, which then can focus on
implementing this short list of primitives in an efficient manner.
Furthermore to support gradient based training one can associate to each
primitive a way to compute the between the output and input cotangent
spaces, which in practice means that the number of supported operations
has to be doubled. In other words operations come in pairs.</p>
<p>One such pair is addition and copying <span class="math display">
\begin{aligned}
+ &amp;\colon \mathbf{R} \times \mathbf{R} \to \mathbf{R}, (x,y) \mapsto
x + y, \\
\Delta &amp;\colon \mathbf{R} \to \mathbf{R} \times \mathbf{R}, dz
\mapsto (dz,dz)
\end{aligned}
</span> another is multiplication <span class="math inline">m</span> and
an operation <span class="math inline">m^*</span> <span
class="math display">
\begin{aligned}
m &amp;\colon \mathbf{R} \times \mathbf{R} \to \mathbf{R}, (x,y) \mapsto
x \cdot y, \\
m^* &amp;\colon (\mathbf{R} \times \mathbf{R}) \times \mathbf{R} \to
\mathbf{R} \times \mathbf{R}, ((x,y), dz) \mapsto (y \cdot dz, x \cdot
dz).
\end{aligned}
</span> Most generally to any smooth map between (pointed) manifolds
<span class="math display">
f \colon (M, p) \to (N, f(p))
</span> we can associate the map between cotangent spaces <span
class="math display">
f^* \colon T^*_{f(p)} N \to T^*_p M.
</span> This turns out to be a functor.</p>
<p>We now want to associate to each of the primitives <span
class="math inline">\phi</span> a <span
class="math inline">e(\phi)</span> that computes <span
class="math inline">\phi</span> and to each of the pullback primitives
<span class="math inline">\phi^*</span> a <span class="math inline">e^*
= e(\phi^*)</span> that computes <span
class="math inline">\phi^*</span>. What is meant concretely by
“processing element” depends on further details. Let us say for example
we wanted to implement primitives as digital circuits. To implement an
addition primitive <span class="math inline">+ \colon \mathbf{R} \times
\mathbf{R} \to \mathbf{R}</span> for example a choice of value
representation must be made, since digital circuits can’t operate on
real numbers and there are then still multiple options to implement the
control and datapath of a digital adder. Other primitives like matrix
multiplication could in principle be regarded as being composed of
elementary operations, but the most efficient implementation of the
overall operation in realistic settings in many cases is not a
composition of implementations of these more primitive operations. In
many ways large parts of neumorphic engineering are concerned with
coming up with novel ways of implementing matrix multiplication
primitives in various physical inncarnations.</p>
<p>Once a choice of implementation is made we want to demand that there
is a notion of composition of the implementation of primitives, which is
compatible with the composition of the primitives. In the case of
artificial neural network primitives this is illustrated in fig. . Each
primitive is computed by a processing element, during the forward pass
the input values need to be stored in a memory. Assuming the memory can
hold only one value, the processing element has to be blocked until
during the backward pass the computation of the pullback consumes the
input value. As it turns out an analogous algorithm can be derived when
each processing element operates in continuous time and operates either
on events or continuous signals.</p>
<figure>
<img src="figures/pe_merge_split.png" id="fig:pe_merge_split"
alt="Figure 2: Illustration of merging (A) and splitting of (B) implemented by processing elements. The merge processing element m_{k,l} implements the primitive \phi \colon \mathbf{R}^k \times \mathbf{R}^l \to \mathbf{R}^{k + l}, (x,y) \mapsto [x_1, \ldots, x_k, y_1, \ldots, y_l] and the element m^*_{l,k} implements the pullback \phi^* \colon \mathbf{R}^{k + l} \to \mathbf{R}^k \times \mathbf{R}^{l}, [x_1, \ldots, x_k, y_1, \ldots, y_l] \mapsto (x,y). Similarly the split processing element s_{k,l} implements the primitive \psi \colon \mathbf{R}^{k + l} \to \mathbf{R}^k \times \mathbf{R}^{l}, [x_1, \ldots, x_k, y_1, \ldots, y_l] \mapsto (x,y). and the element s^*_{l,k} implements the pullback \psi^* \colon \mathbf{R}^k \times \mathbf{R}^l \to \mathbf{R}^{k + l}, (x,y) \mapsto [x_1, \ldots, x_k, y_1, \ldots, y_l]." />
<figcaption aria-hidden="true">Figure 2: Illustration of merging (A) and
splitting of (B) implemented by processing elements. The merge
processing element <span class="math inline">m_{k,l}</span> implements
the primitive <span class="math inline">\phi \colon \mathbf{R}^k \times
\mathbf{R}^l \to \mathbf{R}^{k + l}, (x,y) \mapsto [x_1, \ldots, x_k,
y_1, \ldots, y_l]</span> and the element <span
class="math inline">m^*_{l,k}</span> implements the pullback <span
class="math inline">\phi^* \colon \mathbf{R}^{k + l} \to \mathbf{R}^k
\times \mathbf{R}^{l}, [x_1, \ldots, x_k, y_1, \ldots, y_l] \mapsto
(x,y).</span> Similarly the split processing element <span
class="math inline">s_{k,l}</span> implements the primitive <span
class="math inline">\psi \colon \mathbf{R}^{k + l} \to \mathbf{R}^k
\times \mathbf{R}^{l}, [x_1, \ldots, x_k, y_1, \ldots, y_l] \mapsto
(x,y).</span> and the element <span class="math inline">s^*_{l,k}</span>
implements the pullback <span class="math inline">\psi^* \colon
\mathbf{R}^k \times \mathbf{R}^l \to \mathbf{R}^{k + l}, (x,y) \mapsto
[x_1, \ldots, x_k, y_1, \ldots, y_l]</span>.</figcaption>
</figure>
<p>Informally what we are aiming for is a way of describing the
hierachical composition of time-continuous or discrete time processes,
which “learn” or self-optimise over time. We want to decompose the
problem into two parts: A way to describe the structure of
interconnected systems of elements, that is the legal “diagrams” without
specifying their “dynamics” and then a way to associate a “function” or
“dynamics” to a given diagram. These vague notions have been made
precise in several instances by describing the “structure” of the system
by a suitable <em>operad</em> and the “function” by an <em>operad
algebra</em> <span class="citation" data-cites="spivak:2013"> [<a
href="#ref-spivak:2013" role="doc-biblioref">3</a>]</span>. There the
brain is also explicitely mentioned as one example of a complex
hierachically composed system. Running the program we sketched so far to
completion would therefore mean to first desribe an <em>operad</em> of
neural processing elements and then define an operad algebra which
exhibits “self-optimization” or “learning”, where again these terms
would need to be defined further.</p>
<p>In the context of artificial neural networks a category theoretic
approach has been pursued by <span class="citation"
data-cites="fong2016algebra elliott:2018"> [<a
href="#ref-fong2016algebra" role="doc-biblioref">10</a>,<a
href="#ref-elliott:2018" role="doc-biblioref">11</a>]</span>. Without an
emphasis on learning, various “networks” of components have been
described in this framework as well (Event based systems <span
class="citation" data-cites="zardini2020compositional"> [<a
href="#ref-zardini2020compositional" role="doc-biblioref">1</a>]</span>
, hybrid systems <span class="citation"
data-cites="lerman2020networks"> [<a href="#ref-lerman2020networks"
role="doc-biblioref">2</a>]</span>, open dynamical systems <span
class="citation"
data-cites="vagner2014algebras lerman2016algebra fong2016algebra schultz2020dynamical"> [<a
href="#ref-fong2016algebra" role="doc-biblioref">10</a>,<a
href="#ref-vagner2014algebras" role="doc-biblioref">12</a>–<a
href="#ref-schultz2020dynamical" role="doc-biblioref">14</a>]</span>).
In the context of physics such wiring diagrams were of course first
considered by Feynman to describe probability amplitutes in quantum
mechanics and later QED. Specifying Feynman rules corresponds to the
specification of an operad algebra, indeed this algebraic view in the
context of Feynman diagrams is well known <span class="citation"
data-cites="connes1999hopf baez2011prehistory"> [<a
href="#ref-connes1999hopf" role="doc-biblioref">15</a>,<a
href="#ref-baez2011prehistory" role="doc-biblioref">16</a>]</span>.</p>
<figure>
<img src="figures/pe_optimisation.png" id="fig:pe_optimisation"
alt="Figure 3: Sequential composition of “self-optimizing” or “neural” processing elements. The processing elements e_1, e_2 compute parameterised primitive functions \phi_1 \colon X \times P_1 \to Y, \phi_2 \colon Y \times P_2 \to Z. The parameter values p_1, p_2 (solid light and dark pink) are stored in additional parameter memories. During the forward pass an input message with value x \in X arrives for e_1 at time t_0. The processing element p_1 computes a message with value y = \phi_1(x, p_1) (solid red circle) and stores the input and current parameter value (x, p_1) in memory m_1 at t_1. Similarly at time t_2 the processing element e_2 computes z = \phi_2(y, p_2) and stores (y, p_2) in memory m_2. During the backward pass an error signal (open blue circle) arrives for processing element e^*_2 at t_2, which implements the pullback \phi^*_2. It computes (dy, dp_2) = (\phi^*_2)_{(y,p_2)}(dz), a message with value dy \in T^*_{y} Y (open red circle) is passed to e^*_1 and a message with value (p_2, dp_2) \in P_2 \times T^*_{p_2} P_2 (solid dark pink and open dark pink circle) is passed to the optimizer o_2 at t_1. Finally at t_0 the optimizer o_2 computes a new value p&#39;_2 = f_{p_2}(dp_2) and stores it in the parameter memory and the processing element e^*_1 computes (dx, dp_1) = (\phi^*_1)_{(x,p_1)}(dy) (open green and pink circles)." />
<figcaption aria-hidden="true">Figure 3: Sequential composition of
“self-optimizing” or “neural” processing elements. The processing
elements <span class="math inline">e_1, e_2</span> compute parameterised
primitive functions <span class="math inline">\phi_1 \colon X \times P_1
\to Y, \phi_2 \colon Y \times P_2 \to Z.</span> The parameter values
<span class="math inline">p_1, p_2</span> (solid light and dark pink)
are stored in additional parameter memories. During the <em>forward
pass</em> an input message with value <span class="math inline">x \in
X</span> arrives for <span class="math inline">e_1</span> at time <span
class="math inline">t_0</span>. The processing element <span
class="math inline">p_1</span> computes a message with value <span
class="math inline">y = \phi_1(x, p_1)</span> (solid red circle) and
stores the input and current parameter value <span
class="math inline">(x, p_1)</span> in memory <span
class="math inline">m_1</span> at <span class="math inline">t_1</span>.
Similarly at time <span class="math inline">t_2</span> the processing
element <span class="math inline">e_2</span> computes <span
class="math inline">z = \phi_2(y, p_2)</span> and stores <span
class="math inline">(y, p_2)</span> in memory <span
class="math inline">m_2</span>. During the <em>backward</em> pass an
error signal (open blue circle) arrives for processing element <span
class="math inline">e^*_2</span> at <span
class="math inline">t_2</span>, which implements the pullback <span
class="math inline">\phi^*_2</span>. It computes <span
class="math inline">(dy, dp_2) = (\phi^*_2)_{(y,p_2)}(dz)</span>, a
message with value <span class="math inline">dy \in T^*_{y} Y</span>
(open red circle) is passed to <span class="math inline">e^*_1</span>
and a message with value <span class="math inline">(p_2, dp_2) \in P_2
\times T^*_{p_2} P_2</span> (solid dark pink and open dark pink circle)
is passed to the optimizer <span class="math inline">o_2</span> at <span
class="math inline">t_1</span>. Finally at <span
class="math inline">t_0</span> the optimizer <span
class="math inline">o_2</span> computes a new value <span
class="math inline">p&#39;_2 = f_{p_2}(dp_2)</span> and stores it in the
parameter memory and the processing element <span
class="math inline">e^*_1</span> computes <span class="math inline">(dx,
dp_1) = (\phi^*_1)_{(x,p_1)}(dy)</span> (open green and pink
circles).</figcaption>
</figure>
<hr />
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-zardini2020compositional" class="csl-entry"
role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">G.
Zardini, D. I. Spivak, A. Censi, and E. Frazzoli, <em>A Compositional
Sheaf-Theoretic Framework for Event-Based Systems</em>, arXiv Preprint
arXiv:2005.04715 (2020).</div>
</div>
<div id="ref-lerman2020networks" class="csl-entry"
role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">E.
Lerman and J. Schmidt, <em>Networks of Hybrid Open Systems</em>, Journal
of Geometry and Physics <strong>149</strong>, 103582 (2020).</div>
</div>
<div id="ref-spivak:2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">D.
I. Spivak, <em><a href="http://arxiv.org/abs/1305.0297">The Operad of
Wiring Diagrams: Formalizing a Graphical Language for Databases,
Recursion, and Plug-and-Play Circuits</a></em>, CoRR
<strong>abs/1305.0297</strong>, (2013).</div>
</div>
<div id="ref-he2016deep" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">K.
He, X. Zhang, S. Ren, and J. Sun, <em>Deep Residual Learning for Image
Recognition</em>, in <em>Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em> (2016), pp. 770–778.</div>
</div>
<div id="ref-vaswani2017attention" class="csl-entry"
role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">A.
Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł.
Kaiser, and I. Polosukhin, <em>Attention Is All You Need</em>, Advances
in Neural Information Processing Systems <strong>30</strong>, 5998
(2017).</div>
</div>
<div id="ref-linnainmaa:1970" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">S.
Linnainmaa, The Representation of the Cumulative Rounding Error of an
Algorithm as a <span>Taylor</span> Expansion of the Local Rounding
Errors, Master’s thesis, Univ. Helsinki, 1970.</div>
</div>
<div id="ref-linnainmaa:1976" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">S.
Linnainmaa, <em>Taylor Expansion of the Accumulated Rounding Error</em>,
BIT Numerical Mathematics <strong>16</strong>, 146 (1976).</div>
</div>
<div id="ref-rumelhart:86" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">D.
E. Rumelhart, G. E. Hinton, and R. J. Williams, <em>Learning Internal
Representations by Error Propagation</em>, in <em>Parallel Distributed
Processing</em>, edited by D. E. Rumelhart and J. L. McClelland, Vol. 1
(MIT Press, 1986), pp. 318–362.</div>
</div>
<div id="ref-tensorflow2015-whitepaper" class="csl-entry"
role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">M.
Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A.
Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur,
J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M.
Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V.
Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M.
Wattenberg, M. Wicke, Y. Yu, and X. Zheng, <em><a
href="https://www.tensorflow.org/"><span>TensorFlow</span>: Large-Scale
Machine Learning on Heterogeneous Systems</a></em>, (2015).</div>
</div>
<div id="ref-fong2016algebra" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">B.
Fong, <em><a href="https://arxiv.org/abs/1609.05382">The Algebra of Open
and Interconnected Systems</a></em>, (2016).</div>
</div>
<div id="ref-elliott:2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">C.
Elliott, <em>The Simple Essence of Automatic Differentiation</em>,
Proceedings of the ACM on Programming Languages <strong>2</strong>, 70
(2018).</div>
</div>
<div id="ref-vagner2014algebras" class="csl-entry"
role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">D.
Vagner, D. I. Spivak, and E. Lerman, <em>Algebras of Open Dynamical
Systems on the Operad of Wiring Diagrams</em>, arXiv Preprint
arXiv:1408.1598 (2014).</div>
</div>
<div id="ref-lerman2016algebra" class="csl-entry"
role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">E.
Lerman and D. I. Spivak, <em>An Algebra of Open Continuous Time
Dynamical Systems and Networks</em>, arXiv arXiv (2016).</div>
</div>
<div id="ref-schultz2020dynamical" class="csl-entry"
role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">P.
Schultz, D. I. Spivak, and C. Vasilakopoulou, <em>Dynamical Systems and
Sheaves</em>, Applied Categorical Structures <strong>28</strong>, 1
(2020).</div>
</div>
<div id="ref-connes1999hopf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">A.
Connes and D. Kreimer, <em>Hopf Algebras, Renormalization and
Noncommutative Geometry</em>, in <em>Quantum Field Theory: Perspective
and Prospective</em> (Springer, 1999), pp. 59–109.</div>
</div>
<div id="ref-baez2011prehistory" class="csl-entry"
role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">J.
C. Baez and A. Lauda, <em>A Prehistory of n-Categorical Physics</em>,
Deep Beauty: Understanding the Quantum World Through Mathematical
Innovation 13 (2011).</div>
</div>
</div>
</section>
</article>
</body>
</html>
