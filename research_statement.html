<!DOCTYPE html>
<html lang="en">
  <title>
    Research Statement
  </title>
  <meta name="author" content="Christian Pehle">
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://unpkg.com/tachyons/css/tachyons.min.css">
  <body class="sans-serif">
    <nav class="pa3 pa4-ns">
        <a class="link dim black   f6 f5-ns dib mr3" href="/" title="Home">About</a>
        <a class="link dim gray    f6 f5-ns dib mr3" href="/#projects" title="Projects">Projects</a>
        <a class="link dim gray    f6 f5-ns dib" href="/#contact" title="Contact">Contact</a>
    </nav>
    <section class="ph3 ph4-ns sans-serif" id="research">     
        <article class="measure-wide black-80 serif lh-copy">
            <h2 class="f2 sans-serif">Research Statement</h2>

<p>
My research interests lie at the intersection of machine learning, computational neuroscience, neuromorphic computing, and theoretical physics (typically not all of those intersect at the same time), with a current focus on physical computation. Specifically, I am interested in understanding the physical principles behind "self-optimization" or "learning" in physical systems, particularly in networks of biological neurons. To this end, I am developing software tools for differentiable simulation of interacting physical systems and investigating the theoretical foundations of learning algorithms for these systems using methods from optimal control theory and physics. The ultimate goal is to discover "biologically plausible" learning dynamics and network architectures that can be efficiently implemented and achieve competitive performance compared to current deep learning architectures, with the aim of contributing to the development of general artificial intelligence.
</p>
<p>
During my PhD, I worked on learning algorithms for spiking neural networks, collaborating with Timo Wunderlich to derive the EventProp algorithm - an event-based method for computing exact gradients in networks of spiking neurons, similar to the backpropagation algorithm for artificial neural networks. No such general algorithm was previously known. Besides its theoretically appealing properties it also lends itself to an efficient implementation in event-based software simulators and digital neuromorphic hardware, since both the forward propagation and gradient computation are event-based. This results in order of magnitudes improvement in memory efficiency and runtime performance compared to other approaches. The derivation was based on the adjoint sensitivity method for hybrid dynamical systems from the optimal control theory literature. I am currently exploring both applications of this work to hardware-in-the-loop learning for analog neuromorphic hardware, as well as extensions to a more general class of dynamical systems coupled by events. 
</p>
</article>
</section>
</body>
</html>
